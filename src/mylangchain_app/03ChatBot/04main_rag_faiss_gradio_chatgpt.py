# poetry add gradio_pdf
# í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
import os
from dotenv import load_dotenv

load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
print(OPENAI_API_KEY[30:])

# API í‚¤ ê²€ì¦
if not OPENAI_API_KEY:
    raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

# langchain íŒ¨í‚¤ì§€
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
import gradio as gr

# RAG Chain êµ¬í˜„ì„ ìœ„í•œ íŒ¨í‚¤ì§€
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain

# gradio ì¸í„°í˜ì´ìŠ¤ë¥¼ ìœ„í•œ íŒ¨í‚¤ì§€
from gradio_pdf import PDF

# ì „ì—­ ë³€ìˆ˜ë¡œ ë²¡í„° ì €ì¥ì†Œ ê´€ë¦¬ (ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´)
current_vectorstore = None
current_pdf_path = None

# pdf íŒŒì¼ì„ ì½ì–´ì„œ ë²¡í„° ì €ì¥ì†Œì— ì €ì¥
def load_pdf_to_vector_store(pdf_file, chunk_size=1000, chunk_overlap=100):
    try:
        print(f"PDF íŒŒì¼ ë¡œë”© ì¤‘: {pdf_file}")
        
        # PDF íŒŒì¼ ë¡œë”©
        loader = PyPDFLoader(pdf_file)
        documents = loader.load()
        
        if not documents:
            raise ValueError("PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        print(f"ì´ {len(documents)}í˜ì´ì§€ ë¡œë“œë¨")

        # í…ìŠ¤íŠ¸ ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=int(chunk_size), 
            chunk_overlap=int(chunk_overlap),
            separators=["\n\n", "\n", ".", " ", ""]  # ë” ìì—°ìŠ¤ëŸ¬ìš´ ë¶„í• ì„ ìœ„í•´
        )
        splits = text_splitter.split_documents(documents)
        print(f"ì´ {len(splits)}ê°œ ì²­í¬ë¡œ ë¶„í• ë¨")

        # ì„ë² ë”© ëª¨ë¸ ìƒì„±
        embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY, model="text-embedding-3-small")
        
        # FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„± (ë°°ì¹˜ ì²˜ë¦¬ ë¶ˆí•„ìš”)
        print("FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘...")
        vectorstore = FAISS.from_documents(
            documents=splits, 
            embedding=embeddings
        )
        
        print("ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì™„ë£Œ!")
        return vectorstore
        
    except Exception as e:
        print(f"PDF ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        raise e


# ë²¡í„° ì €ì¥ì†Œì—ì„œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³  ë‹µë³€ì„ ìƒì„±
def retrieve_and_generate_answers(vectorstore, message, temperature=0.5):
    try:
        # ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ retriever ì„¤ì •
        retriever = vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 6}  # ë” ë§ì€ ë¬¸ì„œ ê²€ìƒ‰
        )

        # í•œêµ­ì–´ì— ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸
        system_template = '''ë‹¤ìŒ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”. 
        ë¬¸ë§¥ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, "ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•´ì£¼ì„¸ìš”.

        <ë¬¸ë§¥>
        {context}
        </ë¬¸ë§¥>

        ì§ˆë¬¸: {input}

        ë‹µë³€ ê·œì¹™:
        1. ë¬¸ì„œ ë‚´ìš©ë§Œì„ ê·¼ê±°ë¡œ ë‹µë³€í•˜ì„¸ìš”
        2. ë‹¨ê³„ë³„ ì„¤ëª…ì´ í•„ìš”í•˜ë©´ ìˆœì„œëŒ€ë¡œ ì‘ì„±í•˜ì„¸ìš”  
        3. êµ¬ì²´ì ì¸ ë©”ë‰´ëª…, ë²„íŠ¼ëª…ì„ í¬í•¨í•˜ì„¸ìš”
        4. ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ëŠ” "ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  í•˜ì„¸ìš”

        ë‹µë³€:'''

        prompt = ChatPromptTemplate.from_messages([
            ("system", system_template),
            ("human", "{input}")
        ])

        # ChatModel ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        model = ChatOpenAI(
            #model='gpt-4o-mini', 
            model='gpt-3.5-turbo', 
            temperature=float(temperature),
            api_key=OPENAI_API_KEY
        )

        # Promptì™€ ChatModelì„ Chainìœ¼ë¡œ ì—°ê²°
        document_chain = create_stuff_documents_chain(model, prompt)

        # Retrieverë¥¼ Chainì— ì—°ê²°
        rag_chain = create_retrieval_chain(retriever, document_chain)

        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ ìƒì„±
        response = rag_chain.invoke({'input': message})

        return response['answer']
        
    except Exception as e:
        return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"


# Gradio ì¸í„°í˜ì´ìŠ¤ì—ì„œ ì‚¬ìš©í•  í•¨ìˆ˜
def process_pdf_and_answer(message, history, pdf_file, chunk_size, chunk_overlap, temperature):
    global current_vectorstore, current_pdf_path
    
    # ì…ë ¥ ê²€ì¦
    if not pdf_file:
        return "PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
    
    if not message.strip():
        return "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."
    
    try:
        # PDF íŒŒì¼ì´ ë³€ê²½ë˜ì—ˆê±°ë‚˜ ì²˜ìŒ ë¡œë“œí•˜ëŠ” ê²½ìš°ì—ë§Œ ë²¡í„° ì €ì¥ì†Œ ì¬ìƒì„±
        if current_vectorstore is None or current_pdf_path != pdf_file:
            print("ìƒˆë¡œìš´ PDF íŒŒì¼ ì²˜ë¦¬ ì¤‘...")
            current_vectorstore = load_pdf_to_vector_store(
                pdf_file, chunk_size, chunk_overlap
            )
            current_pdf_path = pdf_file
            print("PDF ì²˜ë¦¬ ì™„ë£Œ!")
        else:
            print("ê¸°ì¡´ ë²¡í„° ì €ì¥ì†Œ ì‚¬ìš©")

        # ë‹µë³€ ìƒì„±
        answer = retrieve_and_generate_answers(current_vectorstore, message, temperature)
        
        return answer
        
    except Exception as e:
        error_msg = f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        print(error_msg)
        return error_msg


# Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±
def create_interface():
    with gr.Blocks(title="PDF ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ") as demo:
        gr.Markdown("# PDF ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ")
        gr.Markdown("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì§ˆë¬¸í•˜ë©´ AIê°€ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ë“œë¦½ë‹ˆë‹¤.")
        
        with gr.Row():
            with gr.Column(scale=1):
                pdf_input = PDF(label="PDF íŒŒì¼ ì—…ë¡œë“œ")
                
                with gr.Accordion("ê³ ê¸‰ ì„¤ì •", open=False):
                    chunk_size = gr.Number(
                        label="ì²­í¬ í¬ê¸°", 
                        value=1000, 
                        info="í…ìŠ¤íŠ¸ë¥¼ ë‚˜ëˆ„ëŠ” ë‹¨ìœ„ (500-2000 ê¶Œì¥)"
                    )
                    chunk_overlap = gr.Number(
                        label="ì²­í¬ ì¤‘ë³µ", 
                        value=200, 
                        info="ì²­í¬ ê°„ ì¤‘ë³µë˜ëŠ” ë¬¸ì ìˆ˜ (50-300 ê¶Œì¥)"
                    )
                    temperature = gr.Slider(
                        label="ì°½ì˜ì„± ìˆ˜ì¤€", 
                        minimum=0, 
                        maximum=1, 
                        step=0.1, 
                        value=0.0,
                        info="0: ì •í™•ì„± ìš°ì„ , 1: ì°½ì˜ì„± ìš°ì„ "
                    )
            
            with gr.Column(scale=2):
                chatbot = gr.Chatbot(label="ğŸ’¬ ëŒ€í™”", height=500)
                msg = gr.Textbox(
                    label="ì§ˆë¬¸ ì…ë ¥", 
                    placeholder="PDF ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”...",
                    lines=2
                )
                
                with gr.Row():
                    submit_btn = gr.Button("ğŸ“¤ ì§ˆë¬¸í•˜ê¸°", variant="primary")
                    clear_btn = gr.Button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”")
        
        # ì˜ˆì‹œ ì§ˆë¬¸ë“¤
        gr.Markdown("### ì§ˆë¬¸ ì˜ˆì‹œ")
        example_questions = [
            "ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.",
            "ì´ ë¬¸ì„œì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ í•µì‹¬ ì‚¬í•­ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "ë¬¸ì„œì— í¬í•¨ëœ ì£¼ìš” ì ˆì°¨ë‚˜ ë‹¨ê³„ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”."
        ]
        
        example_buttons = []
        with gr.Row():
            for question in example_questions:
                btn = gr.Button(question, size="sm")
                example_buttons.append(btn)
        
        # ì´ë²¤íŠ¸ ì²˜ë¦¬
        def respond(message, chat_history, pdf_file, chunk_size, chunk_overlap, temperature):
            if not message.strip():
                return chat_history, ""
            
            # ë‹µë³€ ìƒì„±
            bot_message = process_pdf_and_answer(
                message, chat_history, pdf_file, chunk_size, chunk_overlap, temperature
            )
            
            # ì±„íŒ… íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            chat_history.append((message, bot_message))
            # chat_history.append({"role": "user", "content": message})  # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            # chat_history.append({"role": "assistant", "content": bot_message})  # ë´‡ ì‘ë‹µ ì¶”ê°€

            return chat_history, ""
        
        # ë²„íŠ¼ ì´ë²¤íŠ¸ ì—°ê²°
        submit_btn.click(
            respond, 
            [msg, chatbot, pdf_input, chunk_size, chunk_overlap, temperature], 
            [chatbot, msg]
        )
        
        msg.submit(
            respond, 
            [msg, chatbot, pdf_input, chunk_size, chunk_overlap, temperature], 
            [chatbot, msg]
        )
        
        clear_btn.click(lambda: ([], ""), outputs=[chatbot, msg])
        
        # ì˜ˆì‹œ ì§ˆë¬¸ ë²„íŠ¼ë“¤
        for i, btn in enumerate(example_buttons):
            btn.click(
                lambda q=example_questions[i]: q,
                outputs=msg
            )
    
    return demo

# ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰
if __name__ == "__main__":
    demo = create_interface()
    demo.launch(
        share=False,  # ë¡œì»¬ì—ì„œë§Œ ì‹¤í–‰
        debug=True,   # ë””ë²„ê·¸ ëª¨ë“œ
        server_name="127.0.0.1",  # ë¡œì»¬ ì ‘ì†ë§Œ í—ˆìš©
        server_port=7860
    )